# GCP Financial Data Pipeline

> **End-to-end financial data processing pipeline using Google Cloud Platform services**

##  Architecture Overview

```
Alpha Vantage API â†’ Apache NiFi â†’ GCS â†’ Cloud Function â†’ Dataproc/PySpark â†’ BigQuery + GCS Backup
```

**Data Flow:**
1. **Extract**: Apache NiFi pulls financial data from Alpha Vantage API
2. **Store**: Raw data lands in Google Cloud Storage buckets
3. **Trigger**: Cloud Function monitors bucket changes and triggers processing
4. **Transform**: PySpark jobs on Dataproc clean and transform data
5. **Load**: Processed data flows into BigQuery for analytics + backup to GCS

##  Technologies Used

- **Infrastructure**: GCP Deployment Manager (IaC)
- **Data Extraction**: Apache NiFi on Google Compute Engine
- **Event Processing**: Google Cloud Functions
- **Data Processing**: Apache Spark on Google Dataproc
- **Analytics Storage**: Google BigQuery
- **Object Storage**: Google Cloud Storage
- **Orchestration**: Dataproc Workflow Templates

## ğŸ“ Repository Structure

```
gcp-financial-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ environment-variables.template
â”‚   â””â”€â”€ gcp-config.yaml
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ deployment-manager/
â”‚   â”‚   â”œâ”€â”€ main.yaml
â”‚   â”‚   â”œâ”€â”€ resources/
â”‚   â”‚   â”‚   â”œâ”€â”€ gcs-buckets.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ bigquery-dataset.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ compute-instance.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ iam-roles.yaml
â”‚   â”‚   â”‚   â””â”€â”€ cloud-functions.yaml
â”‚   â”‚   â””â”€â”€ properties/
â”‚   â”‚       â”œâ”€â”€ dev.yaml
â”‚   â”‚       â””â”€â”€ prod.yaml
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ deploy.sh
â”‚       â”œâ”€â”€ cleanup.sh
â”‚       â””â”€â”€ setup-project.sh
â”œâ”€â”€ nifi/
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ alpha-vantage-to-gcs.xml
â”‚   â”‚   â””â”€â”€ market-data-flow.xml
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â””â”€â”€ custom-processors/
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â”œâ”€â”€ install-nifi.sh
â”‚   â”‚   â”œâ”€â”€ configure-nifi.sh
â”‚   â”‚   â””â”€â”€ nifi.properties.template
â”‚   â””â”€â”€ flows/
â”‚       â””â”€â”€ financial-data-ingestion.json
â”œâ”€â”€ cloud-functions/
â”‚   â”œâ”€â”€ gcs-trigger/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ deploy.sh
â”‚   â”‚   â””â”€â”€ function-config.yaml
â”‚   â””â”€â”€ dataproc-launcher/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ deploy.sh
â”œâ”€â”€ dataproc/
â”‚   â”œâ”€â”€ pyspark-jobs/
â”‚   â”‚   â”œâ”€â”€ financial-data-transformer.py
â”‚   â”‚   â”œâ”€â”€ technical-indicators.py
â”‚   â”‚   â”œâ”€â”€ data-quality-checks.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ data_utils.py
â”‚   â”‚       â””â”€â”€ gcs_utils.py
â”‚   â”œâ”€â”€ workflow-templates/
â”‚   â”‚   â”œâ”€â”€ financial-processing-workflow.yaml
â”‚   â”‚   â””â”€â”€ daily-batch-processing.yaml
â”‚   â”œâ”€â”€ cluster-configs/
â”‚   â”‚   â”œâ”€â”€ standard-cluster.yaml
â”‚   â”‚   â””â”€â”€ preemptible-cluster.yaml
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ submit-job.sh
â”‚       â””â”€â”€ create-cluster.sh
â”œâ”€â”€ bigquery/
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ raw_stock_data.json
â”‚   â”‚   â”œâ”€â”€ processed_stock_data.json
â”‚   â”‚   â””â”€â”€ technical_indicators.json
â”‚   â”œâ”€â”€ sql/
â”‚   â”‚   â”œâ”€â”€ create-tables.sql
â”‚   â”‚   â”œâ”€â”€ data-transformations.sql
â”‚   â”‚   â””â”€â”€ analytics-queries.sql
â”‚   â””â”€â”€ views/
â”‚       â”œâ”€â”€ daily-market-summary.sql
â”‚       â””â”€â”€ portfolio-analytics.sql
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ logging/
â”‚   â”‚   â””â”€â”€ log-config.yaml
â”‚   â”œâ”€â”€ alerting/
â”‚   â”‚   â””â”€â”€ alert-policies.yaml
â”‚   â””â”€â”€ dashboards/
â”‚       â””â”€â”€ pipeline-monitoring-dashboard.json
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_data_transformation.py
â”‚   â”‚   â””â”€â”€ test_gcs_utils.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ test_pipeline_e2e.py
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ sample-market-data.json
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ setup-guide.md
â”‚   â”œâ”€â”€ architecture-diagram.png
â”‚   â”œâ”€â”€ troubleshooting.md
â”‚   â”œâ”€â”€ api-documentation.md
â”‚   â””â”€â”€ cost-optimization.md
â””â”€â”€ scripts/
    â”œâ”€â”€ data-validation.py
    â”œâ”€â”€ pipeline-status.sh
    â””â”€â”€ emergency-shutdown.sh
```

##  Quick Start

### Prerequisites
- Google Cloud Platform account with billing enabled
- gcloud CLI installed and configured
- Python 3.8+ installed
- Alpha Vantage API key (store in environment variables)


##  Data Sources

- **Primary**: Alpha Vantage API (stocks, forex, crypto, technical indicators)
- **Frequency**: Daily batch processing with real-time capabilities
- **Volume**: 500+ stocks, 20+ years historical data (~2.5M+ records)
- **Format**: JSON/CSV ingestion, Parquet storage, BigQuery analytics

##  Pipeline Components

### 1. Data Ingestion (Apache NiFi)
- Scheduled pulls from Alpha Vantage API
- Data validation and error handling
- Automatic retry mechanisms
- Rate limiting compliance

### 2. Data Processing (PySpark on Dataproc)
- Data cleaning and normalization
- Technical indicator calculations
- Data quality checks
- Partitioning and optimization

### 3. Analytics Layer (BigQuery)
- Time series analysis ready
- Pre-aggregated views for performance
- Real-time query capabilities
- Cost-optimized table structures

##  Security & Best Practices

- All secrets managed via GCP Secret Manager
- IAM roles follow principle of least privilege
- Data encrypted in transit and at rest
- Audit logging enabled for all components
- Network security via VPC and firewall rules

##  Cost Optimization

- Preemptible instances for Dataproc clusters
- Lifecycle policies for GCS buckets
- BigQuery slot reservations for predictable workloads
- Automatic scaling and shutdown policies

##  Monitoring & Alerting

- Comprehensive logging via Cloud Logging
- Custom metrics and dashboards
- Automated alerting for pipeline failures
- Performance monitoring and optimization


## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.


